{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ff3dee-327c-45d3-aafb-6b8d2d956816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b3899e-90ab-4aa8-8037-35c4909c7b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bd821f-6a0a-42c0-b344-26f879d3963e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'S',\n",
       " '##yl',\n",
       " '##va',\n",
       " '##in',\n",
       " 'and',\n",
       " 'I',\n",
       " 'work',\n",
       " 'at',\n",
       " 'Hu',\n",
       " '##gging',\n",
       " 'Face',\n",
       " 'in',\n",
       " 'Brooklyn',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7599d071-a8a2-4764-8a29-1b5ac34808af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724b7d4333214de494e43c9d530f0ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424dd7a498954970a5e490d3d7c1d155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f1ce6413154c4c9b3c15f2d1e3d04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034abc0dd9eb4e8a80740ec5ad314485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99590707,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67bfbc2-b0dd-4849-b38f-01a0a64a8000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee4a3e7a-64e3-429d-8073-58d7d2c477d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n",
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
      "[[0.9994322657585144, 1.6470283298986033e-05, 3.426703187869862e-05, 1.6042311472119763e-05, 8.250691462308168e-05, 2.1382315026130527e-05, 0.00015649104898329824, 1.9652115952339955e-05, 0.0002208924270235002], [0.9989632368087769, 1.8515740521252155e-05, 5.240453174337745e-05, 1.2534721463453025e-05, 0.0004347368667367846, 3.087430013692938e-05, 0.0003146875824313611, 2.7860676709678955e-05, 0.00014510867185890675], [0.9997084736824036, 8.308119504363276e-06, 2.874564415833447e-05, 5.650359071296407e-06, 8.694857388036326e-05, 9.783468158275355e-06, 6.786145968362689e-05, 1.1793993508035783e-05, 7.241894491016865e-05], [0.9998351335525513, 5.645542842103168e-06, 1.3955181202618405e-05, 4.313377758080605e-06, 4.017695027869195e-05, 8.123079169308767e-06, 5.6485023378627375e-05, 8.991643881017808e-06, 2.7239142582402565e-05], [0.0001833340502344072, 2.5156617994070984e-05, 4.8462032282259315e-05, 1.4900567293807399e-05, 0.9993828535079956, 1.99977403099183e-05, 0.00011153621017001569, 1.0790749911393505e-05, 0.00020288894302211702], [0.0006440280703827739, 7.437902240781114e-05, 0.00013196632789913565, 3.471966920187697e-05, 0.9981549382209778, 3.3829721360234544e-05, 0.0005438188672997057, 1.9978198906756006e-05, 0.000362446706276387], [0.0016408414812758565, 9.469502401771024e-05, 0.00027364405104890466, 4.440649718162604e-05, 0.995907187461853, 5.126226460561156e-05, 0.0012787950690835714, 3.2835516321938485e-05, 0.0006763283163309097], [0.0002290184493176639, 2.5183371690218337e-05, 5.7899578678188846e-05, 9.957038855645806e-06, 0.9992327690124512, 1.7655091141932644e-05, 0.0002344836830161512, 1.235660420206841e-05, 0.00018065057520288974], [0.999804675579071, 5.465036792884348e-06, 1.2950016753165983e-05, 4.9724390009942e-06, 2.3503229385823943e-05, 1.2930728189530782e-05, 9.509925439488143e-05, 8.891771358321421e-06, 3.146939707221463e-05], [0.9995046854019165, 1.4611901860916987e-05, 2.9646907933056355e-05, 8.223412805818953e-06, 0.00016016546578612179, 2.0456824131542817e-05, 0.00017537151870783418, 1.9349154172232375e-05, 6.743992707924917e-05], [0.9996775388717651, 7.5969492172589526e-06, 1.7006908819894306e-05, 3.77761057279713e-06, 6.396068056346849e-05, 1.2297853572817985e-05, 0.00018241393263451755, 7.648388418601826e-06, 2.7664631488732994e-05], [0.999434769153595, 1.1278317288088147e-05, 2.8862716135336086e-05, 6.246597422432387e-06, 8.598271961091086e-05, 2.2989384888205677e-05, 0.0003494521079119295, 1.3841326108376961e-05, 4.654669464798644e-05], [0.018156271427869797, 6.245910481084138e-05, 0.0002693226560950279, 4.766615529661067e-05, 0.0061346180737018585, 0.0002396744239376858, 0.9738931059837341, 7.093795284163207e-05, 0.001125914161093533], [0.014645969495177269, 0.00020479796512518078, 0.0022360309958457947, 9.357065573567525e-05, 0.003731631673872471, 0.0005988667835481465, 0.9761149883270264, 0.00017609840142540634, 0.002198096364736557], [0.0031715717632323503, 8.892182813724503e-05, 0.0015001487918198109, 7.65302829677239e-05, 0.003357573179528117, 0.0004643837455660105, 0.9887976050376892, 0.0001087135897250846, 0.0024346013087779284], [0.9995326995849609, 6.553959792654496e-06, 2.8316144380369224e-05, 6.235935870790854e-06, 3.737203587661497e-05, 2.035713077930268e-05, 0.000287274771835655, 1.5032625924504828e-05, 6.614292215090245e-05], [0.0006589226541109383, 6.671248411294073e-05, 0.00022443967463914305, 4.1903884266503155e-05, 0.0004602030967362225, 9.038783173309639e-05, 0.005088847130537033, 0.0001580320531502366, 0.9932106137275696], [0.9994321465492249, 1.6470659829792567e-05, 3.426761759328656e-05, 1.604244789632503e-05, 8.250887913163751e-05, 2.1382333216024563e-05, 0.00015649327542632818, 1.9652263290481642e-05, 0.00022089366393629462], [0.9994322657585144, 1.6470252376166172e-05, 3.4266999136889353e-05, 1.6042282368289307e-05, 8.250668179243803e-05, 2.1382273189374246e-05, 0.00015649059787392616, 1.965209776244592e-05, 0.00022089200501795858]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)\n",
    "\n",
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13166c81-a665-4724-b890-8d219ba5e2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9556b9ca-044b-4980-96a2-5936f6978de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'O', 'score': 0.9994322657585144, 'word': '[CLS]'}, {'entity': 'O', 'score': 0.9989632368087769, 'word': 'My'}, {'entity': 'O', 'score': 0.9997084736824036, 'word': 'name'}, {'entity': 'O', 'score': 0.9998351335525513, 'word': 'is'}, {'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'}, {'entity': 'O', 'score': 0.999804675579071, 'word': 'and'}, {'entity': 'O', 'score': 0.9995046854019165, 'word': 'I'}, {'entity': 'O', 'score': 0.9996775388717651, 'word': 'work'}, {'entity': 'O', 'score': 0.999434769153595, 'word': 'at'}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face'}, {'entity': 'O', 'score': 0.9995326995849609, 'word': 'in'}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}, {'entity': 'O', 'score': 0.9994321465492249, 'word': '.'}, {'entity': 'O', 'score': 0.9994322657585144, 'word': '[SEP]'}]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    label = model.config.id2label[p]\n",
    "    if label !=\"0\":\n",
    "        result.append(\n",
    "            {\"entity\": label, \"score\": probabilities[i][p], \"word\": tokens[i]}\n",
    "        )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5caf834b-8bd0-41b8-9147-f89023ad1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46a1c78-661d-4d29-b161-bf85dbb8602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'O', 'score': 0.9994322657585144, 'word': '[CLS]', 'start': 0, 'end': [0]}, {'entity': 'O', 'score': 0.9989632368087769, 'word': 'My', 'start': 0, 'end': [2]}, {'entity': 'O', 'score': 0.9997084736824036, 'word': 'name', 'start': 3, 'end': [7]}, {'entity': 'O', 'score': 0.9998351335525513, 'word': 'is', 'start': 8, 'end': [10]}, {'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S', 'start': 11, 'end': [12]}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl', 'start': 12, 'end': [14]}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': [16]}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in', 'start': 16, 'end': [18]}, {'entity': 'O', 'score': 0.999804675579071, 'word': 'and', 'start': 19, 'end': [22]}, {'entity': 'O', 'score': 0.9995046854019165, 'word': 'I', 'start': 23, 'end': [24]}, {'entity': 'O', 'score': 0.9996775388717651, 'word': 'work', 'start': 25, 'end': [29]}, {'entity': 'O', 'score': 0.999434769153595, 'word': 'at', 'start': 30, 'end': [32]}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu', 'start': 33, 'end': [35]}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging', 'start': 35, 'end': [40]}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face', 'start': 41, 'end': [45]}, {'entity': 'O', 'score': 0.9995326995849609, 'word': 'in', 'start': 46, 'end': [48]}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': [57]}, {'entity': 'O', 'score': 0.9994321465492249, 'word': '.', 'start': 57, 'end': [58]}, {'entity': 'O', 'score': 0.9994322657585144, 'word': '[SEP]', 'start': 0, 'end': [0]}]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    label = model.config.id2label[p]\n",
    "    if label !=\"0\":\n",
    "        start, end = offsets[i]\n",
    "        result.append(\n",
    "            {\"entity\": label, \"score\": probabilities[i][p], \"word\": tokens[i], \"start\": start, \"end\": [end]}\n",
    "        )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e99140a7-0023-4cdd-9e37-89d8b7f25c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d5fb14d-d0a4-454a-b7ee-72e3b3326ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c5baae9-bbd2-4ab6-95bf-2eb8c83e1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
      "[True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "print(sequence_ids)\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "print(mask)\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "print(mask)\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b56266c6-57ff-42ea-b0dd-3410991b2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dee34399-b978-43bd-b580-fac50c3f7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "821e0a50-9cd9-4fa2-961a-448894d1e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d63e2439-f710-4168-aa71-8bfe59c51147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac8efadc-acd9-4f7a-af14-82a638e3175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7390b5b6-258e-4c63-9267-28dcad4445d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': tensor(0.9803, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee6de1-3421-4ae5-b816-1947c1d21900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
